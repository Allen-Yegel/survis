@article{Mao2018DeepJ,
  abstract = {Recent advances in deep neural networks have
enabled algorithms to compose music that is comparable to
music composed by humans. However, few algorithms allow the
user to generate music with tunable parameters. The ability to
tune properties of generated music will yield more practical
benefits for aiding artists, filmmakers, and composers in their
creative tasks. In this paper, we introduce DeepJ - an end-to-end
generative model that is capable of composing music conditioned
on a specific mixture of composer styles. Our innovations include
methods to learn musical style and music dynamics. We use our
model to demonstrate a simple technique for controlling the style
of generated music as a proof of concept. Evaluation of our model
using human raters shows that we have improved over the Biaxial
LSTM approach.},
  author       = {Huanru Henry Mao and
                  Taylor Shin and
                  Garrison W. Cottrell\"org},
  doi = {https://doi.org/10.1109/ICSC.2018.00077},
  title        = {DeepJ: Style-Specific Music Generation},
  keywords = {type:music_generation, deep_learning, polyphonic_music, style_conditioning, neural_networks, dynamics_modeling},
  number = {},
  publisher = {IEEE},
  journal      = {CoRR},
  volume       = {abs/1801.00887},
  year         = {2018},
  url          = {http://arxiv.org/abs/1801.00887},
  eprinttype    = {arXiv},
  eprint       = {1801.00887},
  timestamp    = {Mon, 13 Aug 2018 16:48:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1801-00887.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Jiang2019Music,
abstract = {With the development of deep learning, neural
networks are increasingly used in various art fields such as
music, literature and pictures, and even comparable to humans.
This paper proposes a music generation model based on
bidirectional recurrent neural network, which can effectively
explore the complex relationship between notes and obtain the
conditional probability from time and pitch dimensions. The
existing system usually ignored the information in the negative
time direction, however which is non-trivial in the music
prediction task, so we propose a bidirectional LSTM model to
generate the note sequence. Experiments with classical piano
datasets have demonstrated that we achieve high performance
in music generation tasks compared to the existing
unidirectional biaxial LSTM method.},
  author={Jiang, Tianyu and Xiao, Qinyin and Yin, Xueyuan},
  booktitle={2019 IEEE 2nd International Conference on Electronics Technology (ICET)}, 
  title={Music Generation Using Bidirectional Recurrent Network}, 
  year={2019},
  volume={},
  number={},
  pages={564-569},
  keywords={type:recurrent_neural_networks, deep_learning, task_analysis, music, logic_gates, predictive_models, computer_architecture, music_generation, bidirectional_recurrent_neural_network},
  doi={https://doi.org/10.1109/ELTECH.2019.8839399}
}

@INPROCEEDINGS{Zhao2019Emotional,
abstract = {With the development of AI technology in recent
years, Neural Networks have been used in the task of algorithmic
music composition and have achieved desirable results. Music is
highly associated with human emotion, however, there are few
attempts of intelligent music composition in the scene of
expressing different emotions. In this work, Biaxial LSTM
networks have been used to generate polyphonic music, and the
thought of LookBack is also introduced into the architecture to
improve the long-term structure. Above all, we design a novel
system for emotional music generation with a manner of
steerable parameters for 4 basic emotions divided by Russell’s 2-
demonsion valence-arousal (VA) emotional space. The
evaluation indices of generated music by this model is closer to
real music, and via human listening test, it shows that the
different affects expressed by the generated emotional samples
can be distinguished correctly in majority.},
  author={Zhao, Kun and Li, Siqi and Cai, Juanjuan and Wang, Hui and Wang, Jingling},
  booktitle={2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)}, 
  title={An Emotional Symbolic Music Generation System based on LSTM Networks}, 
  year={2019},
  volume={},
  number={},
  pages={2039-2043},
  keywords={type:task_analysis, training, bars, rhythm, recurrent_neural_networks, algorithmic_composition, music_generation, emotional_music, neural_networks, lstm},
  doi={https://doi.org/10.1109/ITNEC.2019.8729266}
}

@INPROCEEDINGS{Lang2020SSCL,
abstract = {Deep neural networks have been used in music related applications such as polyphony music generation, music style transfer and hidden emotions identification. However, it's still challenge to generate long-term music with harmonious structures. In this paper, we propose a Self-similarity Clustering Learning (SSCL) model to meet the above challenge. Our main idea is to reduce the dimensionality of the self-similarity matrix followed by clustering, so that the generated music samples in one cluster have similar long-term strucutres. We built a music dataset by collecting 198 hours music data from Internet to evaluate the quality of the generated music. Experimental results show that our model can synthesize music with better long-term properties which are closer to real music.},
  author={Lang, Runnan and Wu, Songsong and Zhu, Songhao and Li, Zuoyong},
  booktitle={2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)}, 
  title={SSCL: Music Generation in Long-term with Cluster Learning}, 
  year={2020},
  volume={1},
  number={},
  pages={77-81},
  keywords={type:deep_learning, automation, conferences, neural_networks, internet, information_technology, music_generation, lstm, clustering, neural_networks},
  doi={https://doi.org/10.1109/ITNEC48623.2020.9085207}
  }

@inproceedings{T.Liang2017Automatic,
abstract = {This paper presents “BachBot”: an end-to-end automatic
composition system for composing and completing music in the style of Bach’s chorales using a deep long
short-term memory (LSTM) generative model. We propose a new sequential encoding scheme for polyphonic
music and a model for both composition and harmonization which can be efficiently sampled without expensive
Markov Chain Monte Carlo (MCMC). Analysis of the
trained model provides evidence of neurons specializing
without prior knowledge or explicit supervision to detect
common music-theoretic concepts such as tonics, chords,
and cadences. To assess BachBot’s success, we conducted
one of the largest musical discrimination tests on 2336 participants. Among the results, the proportion of responses
correctly differentiating BachBot from Bach was only 1%
better than random guessing.}, 
  author       = {Feynman T. Liang and
                  Mark Gotham and
                  Matthew Johnson 0003 and
                  Jamie Shotton},
  title        = {{Automatic Stylistic Composition of Bach Chorales 
                   with Deep LSTM.}},
  booktitle    = {{Proceedings of the 18th International Society for 
                   Music Information Retrieval Conference}},
  year         = 2018,
  pages        = {449-456},
  publisher    = {ISMIR},
  month        = sep,
  venue        = {Suzhou, China},
  doi          = {10.5281/zenodo.1416208},
  url          = {https://doi.org/10.5281/zenodo.1416208},
  keywords={type:deep_lstm, music_generation, lstm, polyphonic_music, sequential_encoding, music_discrimination_test, harmonization}
}

@inproceedings{Chen2024musicldm,
  abstract = {Diffusion models have shown promising results in cross-modal generation tasks,
including text-to-image and text-to-audio generation.. However, generating music,
as a special type of audio, presents unique challenges due to limited availability of
music data and sensitive issues related to copyright and plagiarism. In this paper,
to tackle these challenges, we first construct a state-of-the-art text-to-music model,
MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music
domain. We achieve this by retraining the contrastive language-audio pretraining
model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a
collection of music data samples. Then, to address the limitations of training
data and to avoid plagiarism, we leverage a beat tracking model and propose two
different mixup strategies for data augmentation: beat-synchronous audio mixup
and beat-synchronous latent mixup, which recombine training audio directly or
via a latent embeddings space, respectively. Such mixup strategies encourage the
model to interpolate between musical training samples and generate new music
within the convex hull of the training data, making the generated music more
diverse while still staying faithful to the corresponding style. In addition to popular
evaluation metrics, we design several new evaluation metrics based on CLAP
score to demonstrate that our proposed MusicLDM and beat-synchronous mixup
strategies improve both the quality and novelty of generated music, as well as the
correspondence between input text and generated music.}, 
  title={MusicLDM: Enhancing novelty in text-to-music generation using beat-synchronous mixup strategies},
  author={Chen, Ke and Wu, Yusong and Liu, Haohe and Nezhurina, Marianna and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1206--1210},
  year={2024},
  organization={IEEE}，
  keywords={type:text-to-music_generation, diffusion_models, beat-synchronous_mixup, musicldm, audio_synthesis, latent_space_interpolation}
}

@article{copet2024simple,
  abstract = {We tackle the task of conditional music generation. We introduce MUSICGEN, a single Language Model (LM) that operates over several streams of compressed discrete
music representation, i.e., tokens. Unlike prior work, MUSICGEN is comprised of
a single-stage transformer LM together with efficient token interleaving patterns,
which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MUSICGEN can generate
high-quality samples, both mono and stereo, while being conditioned on textual
description or melodic features, allowing better controls over the generated output.
We conduct extensive empirical evaluation, considering both automatic and human
studies, showing the proposed approach is superior to the evaluated baselines on a
standard text-to-music benchmark. Through ablation studies, we shed light over
the importance of each of the components comprising MUSICGEN. Music samples,
code, and models are available at github.com/facebookresearch/audiocraft.}, 
  title={Simple and controllable music generation},
  author={Copet, Jade and Kreuk, Felix and Gat, Itai and Remez, Tal and Kant, David and Synnaeve, Gabriel and Adi, Yossi and D{\'e}fossez, Alexandre},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}，
  keywords={type:music_generation, transformer_models, tokenization, autoregressive_models, melody_conditioning, musicgen}
}

@article{garcia2023vampnet,
abstract = {We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule
during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is
non-autoregressive, leveraging a bidirectional transformer
architecture that attends to all tokens in a forward pass.
With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by
prompting VampNet in various ways, we can apply it to
tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style,
genre, instrumentation, and other high-level aspects of the
music. This flexible prompting capability makes VampNet
a powerful music co-creation tool. Code 3
and audio samples 4
are available online.}, 
  title={Vampnet: Music generation via masked acoustic token modeling},
  author={Garcia, Hugo Flores and Seetharaman, Prem and Kumar, Rithesh and Pardo, Bryan},
  journal={arXiv preprint arXiv:2307.04686},
  year={2023},
  keywords={type:masked_acoustic_token_modeling, non-autoregressive_generation, music_synthesis, parallel_iterative_decoding, music_inpainting_and_compression, bidirectional_transformer}
}

@article{lam2024efficient,
abstract = {Recent progress in music generation has been remarkably advanced by the stateof-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for
semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained
acoustic tokens, making it computationally expensive and prohibitive for a realtime generation. Efficient music generation with a quality on par with MusicLM
remains a significant challenge. In this paper, we present MeLoDy (M for music;
L for LM; D for diffusion), an LM-guided diffusion model that generates music
audios of state-of-the-art quality meanwhile reducing 95.7% to 99.6% forward
passes in MusicLM, respectively, for sampling 10s to 30s music. MeLoDy inherits
the highest-level LM from MusicLM for semantic modeling, and applies a novel
dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the
conditioning semantic tokens into waveform. DPD is proposed to simultaneously
model the coarse and fine acoustics by incorporating the semantic information
into segments of latents effectively via cross-attention at each denoising step. Our
experimental results suggest the superiority of MeLoDy, not only in its practical
advantages on sampling speed and infinitely continuable generation, but also in its
state-of-the-art musicality, audio quality, and text correlation.
Our samples are available at https://Efficient-MeLoDy.github.io/.},
  title={Efficient neural music generation},
  author={Lam, Max WY and Tian, Qiao and Li, Tang and Yin, Zongyu and Feng, Siyuan and Tu, Ming and Ji, Yuliang and Xia, Rui and Ma, Mingbo and Song, Xuchen and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024},
  keywords={type:melody, dual-path_diffusion(dpd), semantic_modeling, audio_vae-gan, non-autoregressive_generation, music_continuation_and_variation}
}

@ARTICLE{Imasato2023Generate,
abstract = {This work proposes a transformer-based model capable of generating music in its symbolic domain, in a controllable fashion. The ultimate goal of this is to build a system with which people can compose music collaboratively with a computer. Using an NLP model as a base (GPT-2), we take advantage of the similarities across symbolic music representation and written language to build a model capable of conditionally predicting musical sequences. Controllability is achieved without explicit programming for it, and does not require extensive retraining of the model. A study with 939 participants was performed to evaluate this controllability. The results of this suggest the proposed method is indeed effective and can be used to control the generation of music in its symbolic domain. The method itself is flexible to any desired “control”, but this work focuses specifically on the emotion conveyed when one listens to a piece of music.},
  author={Imasato, Naomi and Miyazawa, Kazuki and Duncan, Caitlin and Nagai, Takayuki},
  journal={IEEE Access}, 
  title={Using a Language Model to Generate Music in Its Symbolic Domain While Controlling Its Perceived Emotion}, 
  year={2023},
  volume={11},
  number={},
  pages={52412-52428},
  keywords={type:transformers, deep_learning, predictive_models, data_models, computational_modeling, artificial_intelligence, computer_generated_music, music, ai_music_composition, controlled_music_generation, language_model, autoregressive_model},
  doi={10.1109/ACCESS.2023.3280603}  
}